---
layout: single
title: "[Data Science Project] Amazon Magazine Subscription Review Data Analysis: Topic Modeling Techniques on LDA, BERTopic, and LLM-based QualIT"
toc: true
categories: project
comments: true
thumbnail: https://gomediashark.com/wp-content/uploads/2024/05/How-to-Get-Amazon-Reviews-the-Right-Way.png
---

# Introduction

Topic modeling is a widely used language processing (NLP) technique for extracting latent thematic structures from unstructured text data, such as social media posts, news articles, or customer feedback. 
This project examines the application of various Topic Modeling approaches, including LDA, BERTopic, and a large language model (LLM), an enhanced method inspired by **QualIT** (Karpoor et al., 2024). 
Based on Amazon Magazine review data, this project performs Topic Modeling to extract keywords and cluster reviews by each characteristic.

![AWS Review](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bv9gl68663y4LYh1IYAmFw.png)

## How do LLMs Work?

LLM represents words using **vector embeddings** and learn to predict the next word in a sequence. During training, the model updates parameters to maximize the  likelihood of correct predictions, 
a process known as self-supervised learning. Once trained, LLMs can:

- Answer questions

- Translate languages

- Summarize documents

- Write code or articles


## Why are LLMs Important?

LLMs power generative AI, assist in **customer service**, support **content creation**, and are redefining **knowledge retrieval** and **developer productivity**.

## Real-World Applications

- Copywriting and marketing content

- Customer sentiment analysis

- Text classification

- Code generation (e.g., GitHub Copilot, CodeWhisperer)

- Conversational AI (e.g., ChatGPT, Alexa)

## The Future of LLMs

LLMs are expected to evolve with:

- Better accuracy and bias reduction

- Multimodal training (text, image, audio)

- Wider workplace automation

- Smarter, more interactive virtual assistants

If you want more information, click this link here: [LLM on AWS](https://aws.amazon.com/what-is/large-language-model/)

# Kapoor et al. (2024)

Topic modeling is a widely used technique for uncovering thematic structures from large text corpora. However, traditional topic modeling approaches have several limitations (explained later). They suggest a new type of topic modeling method, Qualitative Insights Tool (QualIT), that integrates large language models (LLMs) with existing clustering-based topic modeling approaches. Their method influences the deep contextual understanding and powerful language generation capabilities of LLMs to enrich the topic modeling process using clustering. This finding suggests the integration of LLMs can unlock new opportunities for topic modeling of dynamic and complex text data, as is common in talent management research contexts. 

## Research Gap

Most topic modeling approaches (e.g., Latent Dirichlet Allocation (LDA) struggle to capture nuanced semantics and contextual understanding required to accurately model narratives. They also fail to capture the contextual nuances and ambiguities inherent in natural language, as they rely heavily on predefined rules and patterns.

When conducting topic modeling in talent management research, the limitations of topic modeling make its development necessary. These include taking too much time (about 3 months per project) to sample participants, collect data, and analyze documents. Further, the lack of familiarity/expertise in qualitative research methods limits easy analysis or sharing of insights. 

## Methodology

The newly introduced LLM Enhanced Topic Modeling consists of multiple steps to generate the main topics, which are then used to determine subtopics from documents. 

### Key-Phrase Extraction

The LLM prompt can extract multiple key phrases from the document, depending on its content. Alternative methods (e.g., BERTopic) assume that each document only contains a single topic, when in reality, a document may contain more than a single topic. 

### Hallucination Check

Hallucination Check scores how well the key phrase aligns with the actual text. 

$$C_i = 1/n \sum\limits_{j=1}^n (V_input,ij alt183 V_keyphrases,ij)/|V_input,ij| alt183 |V_keyphrases,ij|$$

- $$C_i$$ is the coherence score for the $$i$$-th document
- $$n$$ is the number of dimensions in the embedding space
- $$V_input,ij$$ is the $$j$$-th dimension of the normalized embedding vector for the input text of the $$i$$-th document.
- $$V_keyphrases,ij$$ is the $$j$$-th dimension of the normalized embedding vector for the theme text of the $$i$$-th document.
- $$V_input,ij alt183 V_keyphrases,ij)$$ denotes the dot product of the two vectors.
- $$||v||$$ denotes the Euclidean norm (or length) of vector $$v$$.

### Clustering

They used a partitional k-means clustering algorithm to group the key phrases identified in the previous step. The aim of this step is to group documents into multiple clusters, each representing a collection of documents into multiple clusters, each representing a collection of documents with similar semantic content. 

#### Clustering for Main Topics 



#### Clustering for Sub Topics

## Result + interpretation

# Data introduction

In ``Kapoor et al. (2024)``, they used 

For this project, I utilized customer review data collected by Amazon, specifically focusing on magazine subscriptions. 
These reviews are rich in natural language content and highly suitable for testing topic modeling techniques.

## Data sources

The dataset was obtained from the publicly available UCSD Amazon Review Dataset:

- UCSD Amazon Review Dataset:

[https://cseweb.ucsd.edu/~jmcauley/datasets.html](https://cseweb.ucsd.edu/~jmcauley/datasets.html)

- File used: ``Magazine_Subscriptions.jsonl``

## Major characteristics of the data 

- Fields:

  - ``rating`` (float: from 1.0 to 5.0)
  - ``title`` (Title of the review)
  - ``text`` (main review body)
  - ``asin``, ``timestamp``, and other metadata

- Langauge: ``English``

- Document type: Free-text customer reviews

- Structure: Highly diverse in tone, length, and writing style

- Size: ~71,500 records

This variety in user-generated content makes the dataset particularly challenging and valuable for exploring different topic modeling strategies.


## data pre-processing

To ensure consistency across all models, a **unified preprocessing pipeline** was implemented. The main steps included:

- Removing extra whitespace

- Stripping emails and apostrophes

- Filtering out non-alphabet characters

- Converting all text to lowercase

Hereâ€™s the preprocessing function used:

```python
import re

# Preprocess the text data
def preprocess_text(text):
    text = re.sub('\s+', ' ', str(text))  # Remove extra spaces
    text = re.sub('\S*@\S*\s?', '', str(text))  # Remove emails
    text = re.sub('\'', '', str(text))  # Remove apostrophes
    text = re.sub('[^a-zA-Z]', ' ', str(text))  # Remove non-alphabet characters
    text = text.lower()  # Convert to lowercase
    return text
data['cleaned_text'] = data['text'].apply(preprocess_text)
```
- Tokenized using ``gensim``
- Stopwords removed using ``nltk``
- Lemmatized using WordNet Lemmatizer

# LDA

## What is LDA?

LDA (Latent Dirichlet Allocation) is a type of traditional topic modeling technique. 
It is a generative probabilistic model that operates on the principle that each document in a corpus is composed of a mixture of latent topics, with each topic being represented by a unique probability distribution over the vocabulary. 
This model learns the topic-world distribution by leveraging the co-occurrence patterns of words within the documents, allowing it to uncover the underlying thematic structure of the corpus. 

A key step in the LDA modeling process is determining the appropriate number of topics to be extracted from the data. The choice of the number of topics can have a significant impact on the interpretability and performance of the LDA model, as too few topics may fail to capture the nuances of the data, while too many topics can lead to overfitting and poor generalization. 




<iframe src="/assets/lda_visualization.html" width="100%" height="600px" frameborder="0"></iframe>


# BERTopic

BERTopic is a clustering-based approach that suffers from limitations such as word representation overload or the generation of only one topic per text. 

# LLM (QualIT)

QualIT: Qualitative Insights Tool is presented in ``Kapoor et al. (2024)`` to extend the capabilities of existing topic models. This approach integrates pre-trained LLMs with clustering techniques to systematically address the limitations of both methods and generate more nuanced and interpretable topic representations from free-text data. QualIT complements the analysis step that takes too much time, about 3 months per project, to take one month of a researcher's time per project. 

Unlike traditional topic modeling approaches that rely on hand-crafted features or simple statistical patterns, LLMs can capture complex semantic and syntactic relationships within language, allowing them to better handle the nuances and ambiguities inherent in real-world text. 

# Analysis

## Explanation of analysis

## Showing codes + results

## Interpretation

# Thank you for reading!

I hope this helped you better understand topic modeling with LLMs. If you're curious about NLP, data science, or AI experiments like this one, **subscribe** and stay tuned for more deep dives!

ðŸ’¡ Suggestions or questions? Iâ€™d love to hear from you in the comments!

![ì›€ì§¤](https://upload2.inven.co.kr/upload/2015/05/13/bbs/i11911739460.gif?MW=800)
